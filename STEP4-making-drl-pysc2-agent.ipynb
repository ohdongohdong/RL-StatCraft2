{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4 - Making DRL PySC2 Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Runnning 'Agent code' on jupyter notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "### unfortunately, PySC2 uses Abseil, which treats python code as if its run like an app\n",
    "# This does not play well with jupyter notebook\n",
    "# So we will need to monkeypatch sys.argv\n",
    "\n",
    "\n",
    "import sys\n",
    "#sys.argv = [\"python\", \"--map\", \"AbyssalReef\"]\n",
    "sys.argv = [\"python\", \"--map\", \"Simple64\"]\n",
    "\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS-IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Run an agent.\"\"\"\n",
    "\n",
    "#from __future__ import absolute_import\n",
    "#from __future__ import division\n",
    "#from __future__ import print_function\n",
    "\n",
    "import importlib\n",
    "import threading\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from future.builtins import range  # pylint: disable=redefined-builtin\n",
    "\n",
    "from pysc2 import maps\n",
    "from pysc2.env import available_actions_printer\n",
    "from pysc2.env import run_loop\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import point_flag\n",
    "from pysc2.lib import stopwatch\n",
    "from pysc2.lib import actions\n",
    "from pysc2.lib import features\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# because of Abseil's horrible design for running code underneath Colabs\n",
    "# We have to pull out this ugly hack from the hat\n",
    "if \"flags_defined\" not in globals():\n",
    "    flags.DEFINE_bool(\"render\", True, \"Whether to render with pygame.\")\n",
    "    point_flag.DEFINE_point(\"feature_screen_size\", \"32\",\n",
    "                            \"Resolution for screen feature layers.\")\n",
    "    point_flag.DEFINE_point(\"feature_minimap_size\", \"32\",\n",
    "                            \"Resolution for minimap feature layers.\")\n",
    "    point_flag.DEFINE_point(\"rgb_screen_size\", None,\n",
    "                            \"Resolution for rendered screen.\")\n",
    "    point_flag.DEFINE_point(\"rgb_minimap_size\", None,\n",
    "                            \"Resolution for rendered minimap.\")\n",
    "    flags.DEFINE_enum(\"action_space\", \"RAW\", sc2_env.ActionSpace._member_names_,  # pylint: disable=protected-access\n",
    "                      \"Which action space to use. Needed if you take both feature \"\n",
    "                      \"and rgb observations.\")\n",
    "    flags.DEFINE_bool(\"use_feature_units\", False,\n",
    "                      \"Whether to include feature units.\")\n",
    "    flags.DEFINE_bool(\"use_raw_units\", True,\n",
    "                      \"Whether to include raw units.\")\n",
    "    flags.DEFINE_integer(\"raw_resolution\", 64, \"Raw Resolution.\")\n",
    "    flags.DEFINE_bool(\"disable_fog\", True, \"Whether to disable Fog of War.\")\n",
    "\n",
    "    flags.DEFINE_integer(\"max_agent_steps\", 0, \"Total agent steps.\")\n",
    "    flags.DEFINE_integer(\"game_steps_per_episode\", None, \"Game steps per episode.\")\n",
    "    flags.DEFINE_integer(\"max_episodes\", 0, \"Total episodes.\")\n",
    "    flags.DEFINE_integer(\"step_mul\", 48, \"Game steps per agent step.\")\n",
    "    flags.DEFINE_float(\"fps\", 22.4, \"Frames per second to run the game.\")\n",
    "\n",
    "    #flags.DEFINE_string(\"agent\", \"sc2.agent.BasicAgent.ZergBasicAgent\",\n",
    "    #                    \"Which agent to run, as a python path to an Agent class.\")\n",
    "    #flags.DEFINE_enum(\"agent_race\", \"zerg\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
    "    #                  \"Agent 1's race.\")\n",
    "    flags.DEFINE_string(\"agent\", \"TerranRLAgentWithRawActsAndRawObs\",\n",
    "                        \"Which agent to run, as a python path to an Agent class.\")\n",
    "    flags.DEFINE_enum(\"agent_race\", \"terran\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
    "                      \"Agent 1's race.\")\n",
    "\n",
    "    flags.DEFINE_string(\"agent2\", \"Bot\", \"Second agent, either Bot or agent class.\")\n",
    "    flags.DEFINE_enum(\"agent2_race\", \"terran\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
    "                      \"Agent 2's race.\")\n",
    "    flags.DEFINE_enum(\"difficulty\", \"very_easy\", sc2_env.Difficulty._member_names_,  # pylint: disable=protected-access\n",
    "                      \"If agent2 is a built-in Bot, it's strength.\")\n",
    "\n",
    "    flags.DEFINE_bool(\"profile\", False, \"Whether to turn on code profiling.\")\n",
    "    flags.DEFINE_bool(\"trace\", False, \"Whether to trace the code execution.\")\n",
    "    flags.DEFINE_integer(\"parallel\", 1, \"How many instances to run in parallel.\")\n",
    "\n",
    "    flags.DEFINE_bool(\"save_replay\", True, \"Whether to save a replay at the end.\")\n",
    "\n",
    "    flags.DEFINE_string(\"map\", None, \"Name of a map to use.\")\n",
    "    flags.mark_flag_as_required(\"map\")\n",
    "\n",
    "flags_defined = True\n",
    "\n",
    "def run_thread(agent_classes, players, map_name, visualize):\n",
    "  \"\"\"Run one thread worth of the environment with agents.\"\"\"\n",
    "  with sc2_env.SC2Env(\n",
    "      map_name=map_name,\n",
    "      players=players,\n",
    "      agent_interface_format=sc2_env.parse_agent_interface_format(\n",
    "        #feature_screen=FLAGS.feature_screen_size,\n",
    "        #feature_minimap=FLAGS.feature_minimap_size,\n",
    "        feature_screen=32,\n",
    "        feature_minimap=32,\n",
    "        #feature_dimensions=features.Dimensions(screen=32, minimap=32),\n",
    "        rgb_screen=FLAGS.rgb_screen_size,\n",
    "        rgb_minimap=FLAGS.rgb_minimap_size,\n",
    "        action_space=FLAGS.action_space,\n",
    "        use_raw_units=FLAGS.use_raw_units,\n",
    "        raw_resolution=FLAGS.raw_resolution),\n",
    "      step_mul=FLAGS.step_mul,\n",
    "      game_steps_per_episode=FLAGS.game_steps_per_episode,\n",
    "      disable_fog=FLAGS.disable_fog,\n",
    "      visualize=visualize) as env:\n",
    "    #env = available_actions_printer.AvailableActionsPrinter(env)\n",
    "    agents = [agent_cls() for agent_cls in agent_classes]\n",
    "    run_loop.run_loop(agents, env, FLAGS.max_agent_steps, FLAGS.max_episodes)\n",
    "    if FLAGS.save_replay:\n",
    "      env.save_replay(agent_classes[0].__name__)\n",
    "\n",
    "def main(unused_argv):\n",
    "  \"\"\"Run an agent.\"\"\"\n",
    "  #stopwatch.sw.enabled = FLAGS.profile or FLAGS.trace\n",
    "  #stopwatch.sw.trace = FLAGS.trace\n",
    "\n",
    "  map_inst = maps.get(FLAGS.map)\n",
    "\n",
    "  agent_classes = []\n",
    "  players = []\n",
    "\n",
    "  #agent_module, agent_name = FLAGS.agent.rsplit(\".\", 1)\n",
    "  #agent_cls = getattr(importlib.import_module(agent_module), agent_name)\n",
    "  #agent_classes.append(agent_cls)\n",
    "  agent_classes.append(TerranRLAgentWithRawActsAndRawObs)\n",
    "  players.append(sc2_env.Agent(sc2_env.Race[FLAGS.agent_race]))\n",
    "\n",
    "  if map_inst.players >= 2:\n",
    "    if FLAGS.agent2 == \"Bot\":\n",
    "      players.append(sc2_env.Bot(sc2_env.Race[FLAGS.agent2_race],\n",
    "                                 sc2_env.Difficulty[FLAGS.difficulty]))\n",
    "    else:\n",
    "      #agent_module, agent_name = FLAGS.agent2.rsplit(\".\", 1)\n",
    "      #agent_cls = getattr(importlib.import_module(agent_module), agent_name)\n",
    "      agent_classes.append(TerranRandomAgent)\n",
    "      players.append(sc2_env.Agent(sc2_env.Race[FLAGS.agent2_race]))\n",
    "\n",
    "  threads = []\n",
    "  for _ in range(FLAGS.parallel - 1):\n",
    "    t = threading.Thread(target=run_thread,\n",
    "                         args=(agent_classes, players, FLAGS.map, False))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "\n",
    "  run_thread(agent_classes, players, FLAGS.map, FLAGS.render)\n",
    "\n",
    "  for t in threads:\n",
    "    t.join()\n",
    "\n",
    "  if FLAGS.profile:\n",
    "    pass\n",
    "    #print(stopwatch.sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating a PySC2 Agent with Raw Actions & Observations\n",
    "\n",
    "![StarCraft2 PySC2 interfaces](./images/StarCraft2_PySC2_interfaces.png)\n",
    "\n",
    "ref : https://on-demand.gputechconf.com/gtc/2018/presentation/s8739-machine-learning-with-starcraft-II.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### < PySC2 Interfaces 3가지 종류 >\n",
    "\n",
    "### 1st, Rendered\n",
    "* Decomposed :\n",
    "    - Screen, minimap, resources, available actions\n",
    "* Same control as humans :\n",
    "    - Pixel coordinates\n",
    "    - Move camera\n",
    "    - Select unit/rectangle\n",
    "* Great for Deep Learning, but hard\n",
    "\n",
    "### 2nd, Feature Layer\n",
    "* Same actions : still in pixel space\n",
    "* Same decomposed observations, but more abstract\n",
    "    - Orthogonal camera \n",
    "* Layers:\n",
    "    - unit type\n",
    "    - unit owner\n",
    "    - selection\n",
    "    - health\n",
    "    - unit density\n",
    "    - etc\n",
    "    \n",
    "### 3rd, Raw\n",
    "* List of units and state\n",
    "* Control each unit individually in world coordinates\n",
    "* Gives all observable state (no camera)\n",
    "* Great for scripted agents and programmatic replay analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### < Raw Actions & Observations 을 사용하는 이유>\n",
    "* Raw Actions & Observations 은 world cordinates를 사용하므로 전체 Map을 한번에 관찰하고 Camera를 이동하지 않고도 Map 상의 어느 곳에서도 Action을 취할 수 있는 새로운 형태의 Feature 이다.\n",
    "* 이번 과정에 SL(Supervised Learning, 지도학습)을 활용한 학습은 없지만 스타크래프트 2 리플레이를 활용한 SL은 Raw Actions & Observations를 활용한 \"programmatic replay analysis\"가 필요하다.\n",
    "* 인간 플레이어를 이긴 DeepMind의 AlphaStar의 주요 변경사항 중의 하나는 Raw Actions & Observations 의 활용이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9):\n",
    "        self.actions = actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reward_decay = reward_decay\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation, e_greedy=0.9):\n",
    "        self.check_state_exist(observation)\n",
    "        if np.random.uniform() < e_greedy:\n",
    "            state_action = self.q_table.loc[observation, :]\n",
    "            action = np.random.choice(\n",
    "              state_action[state_action == np.max(state_action)].index)\n",
    "        else:\n",
    "            action = np.random.choice(self.actions)\n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        if s_ != 'terminal':\n",
    "            q_target = r + self.reward_decay * self.q_table.loc[s_, :].max()\n",
    "        else:\n",
    "            q_target = r\n",
    "            \n",
    "        self.q_table.loc[s, a] += self.learning_rate * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            self.q_table = self.q_table.append(pd.Series([0] * len(self.actions), \n",
    "                                                       index=self.q_table.columns, \n",
    "                                                       name=state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranAgentWithRawActsAndRawObs(base_agent.BaseAgent):\n",
    "    actions = (\"do_nothing\",\n",
    "               \"harvest_minerals\",\n",
    "               \"build_supply_depot\",\n",
    "               \"build_barracks\",\n",
    "               \"train_marine\",\n",
    "               \"attack\")\n",
    "\n",
    "    def get_my_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.raw_units\n",
    "                if unit.unit_type == unit_type\n",
    "                and unit.alliance == features.PlayerRelative.SELF]\n",
    "\n",
    "    def get_enemy_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.raw_units\n",
    "                if unit.unit_type == unit_type\n",
    "                and unit.alliance == features.PlayerRelative.ENEMY]\n",
    "\n",
    "    def get_my_completed_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.raw_units\n",
    "                if unit.unit_type == unit_type\n",
    "                and unit.build_progress == 100\n",
    "                and unit.alliance == features.PlayerRelative.SELF]\n",
    "\n",
    "    def get_enemy_completed_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.raw_units\n",
    "                if unit.unit_type == unit_type\n",
    "                and unit.build_progress == 100\n",
    "                and unit.alliance == features.PlayerRelative.ENEMY]\n",
    "\n",
    "    def get_distances(self, obs, units, xy):\n",
    "        units_xy = [(unit.x, unit.y) for unit in units]\n",
    "        return np.linalg.norm(np.array(units_xy) - np.array(xy), axis=1)\n",
    "\n",
    "    def step(self, obs):\n",
    "        super(TerranAgentWithRawActsAndRawObs, self).step(obs)\n",
    "        if obs.first():\n",
    "            command_center = self.get_my_units_by_type(\n",
    "                obs, units.Terran.CommandCenter)[0]\n",
    "            self.base_top_left = (command_center.x < 32)\n",
    "\n",
    "    def do_nothing(self, obs):\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "    def harvest_minerals(self, obs):\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        idle_scvs = [scv for scv in scvs if scv.order_length == 0]\n",
    "        if len(idle_scvs) > 0:\n",
    "            mineral_patches = [unit for unit in obs.observation.raw_units\n",
    "                               if unit.unit_type in [\n",
    "                                   units.Neutral.BattleStationMineralField,\n",
    "                                   units.Neutral.BattleStationMineralField750,\n",
    "                                   units.Neutral.LabMineralField,\n",
    "                                   units.Neutral.LabMineralField750,\n",
    "                                   units.Neutral.MineralField,\n",
    "                                   units.Neutral.MineralField750,\n",
    "                                   units.Neutral.PurifierMineralField,\n",
    "                                   units.Neutral.PurifierMineralField750,\n",
    "                                   units.Neutral.PurifierRichMineralField,\n",
    "                                   units.Neutral.PurifierRichMineralField750,\n",
    "                                   units.Neutral.RichMineralField,\n",
    "                                   units.Neutral.RichMineralField750\n",
    "                               ]]\n",
    "            scv = random.choice(idle_scvs)\n",
    "            distances = self.get_distances(obs, mineral_patches, (scv.x, scv.y))\n",
    "            mineral_patch = mineral_patches[np.argmin(distances)]\n",
    "            return actions.RAW_FUNCTIONS.Harvest_Gather_unit(\n",
    "                \"now\", scv.tag, mineral_patch.tag)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "    def build_supply_depot(self, obs):\n",
    "        supply_depots = self.get_my_units_by_type(obs, units.Terran.SupplyDepot)\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        if (len(supply_depots) == 0 and obs.observation.player.minerals >= 100 and\n",
    "                len(scvs) > 0):\n",
    "            supply_depot_xy = (22, 26) if self.base_top_left else (35, 42)\n",
    "            distances = self.get_distances(obs, scvs, supply_depot_xy)\n",
    "            scv = scvs[np.argmin(distances)]\n",
    "            return actions.RAW_FUNCTIONS.Build_SupplyDepot_pt(\n",
    "                \"now\", scv.tag, supply_depot_xy)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "    def build_barracks(self, obs):\n",
    "        completed_supply_depots = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.SupplyDepot)\n",
    "        barrackses = self.get_my_units_by_type(obs, units.Terran.Barracks)\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        if (len(completed_supply_depots) > 0 and len(barrackses) == 0 and\n",
    "                obs.observation.player.minerals >= 150 and len(scvs) > 0):\n",
    "            barracks_xy = (22, 21) if self.base_top_left else (35, 45)\n",
    "            distances = self.get_distances(obs, scvs, barracks_xy)\n",
    "            scv = scvs[np.argmin(distances)]\n",
    "            return actions.RAW_FUNCTIONS.Build_Barracks_pt(\n",
    "                \"now\", scv.tag, barracks_xy)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "    def train_marine(self, obs):\n",
    "        completed_barrackses = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.Barracks)\n",
    "        free_supply = (obs.observation.player.food_cap -\n",
    "                       obs.observation.player.food_used)\n",
    "        if (len(completed_barrackses) > 0 and obs.observation.player.minerals >= 100\n",
    "                and free_supply > 0):\n",
    "            barracks = self.get_my_units_by_type(obs, units.Terran.Barracks)[0]\n",
    "            if barracks.order_length < 5:\n",
    "                return actions.RAW_FUNCTIONS.Train_Marine_quick(\"now\", barracks.tag)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "    def attack(self, obs):\n",
    "        marines = self.get_my_units_by_type(obs, units.Terran.Marine)\n",
    "        if len(marines) > 0:\n",
    "            attack_xy = (38, 44) if self.base_top_left else (19, 23)\n",
    "            distances = self.get_distances(obs, marines, attack_xy)\n",
    "            marine = marines[np.argmax(distances)]\n",
    "            x_offset = random.randint(-4, 4)\n",
    "            y_offset = random.randint(-4, 4)\n",
    "            return actions.RAW_FUNCTIONS.Attack_pt(\n",
    "                \"now\", marine.tag, (attack_xy[0] + x_offset, attack_xy[1] + y_offset))\n",
    "        return actions.RAW_FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranRandomAgent(TerranAgentWithRawActsAndRawObs):\n",
    "    def step(self, obs):\n",
    "        super(TerranRandomAgent, self).step(obs)\n",
    "        action = random.choice(self.actions)\n",
    "        return getattr(self, action)(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranRLAgentWithRawActsAndRawObs(TerranAgentWithRawActsAndRawObs):\n",
    "    def __init__(self):\n",
    "        super(TerranRLAgentWithRawActsAndRawObs, self).__init__()\n",
    "        self.qlearn = QLearningTable(self.actions)\n",
    "        self.new_game()\n",
    "        self.data_file = 'rlagent_with_raw_acts_and_obs_learning_data'\n",
    "        if os.path.isfile(self.data_file + '.gz'):\n",
    "            self.qlearn.q_table = pd.read_pickle(self.data_file + '.gz', compression='gzip')\n",
    "\n",
    "    def reset(self):\n",
    "        super(TerranRLAgentWithRawActsAndRawObs, self).reset()\n",
    "        self.new_game()\n",
    "\n",
    "    def new_game(self):\n",
    "        self.base_top_left = None\n",
    "        self.previous_state = None\n",
    "        self.previous_action = None\n",
    "\n",
    "    def get_state(self, obs):\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        idle_scvs = [scv for scv in scvs if scv.order_length == 0]\n",
    "        command_centers = self.get_my_units_by_type(obs, units.Terran.CommandCenter)\n",
    "        supply_depots = self.get_my_units_by_type(obs, units.Terran.SupplyDepot)\n",
    "        completed_supply_depots = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.SupplyDepot)\n",
    "        barrackses = self.get_my_units_by_type(obs, units.Terran.Barracks)\n",
    "        completed_barrackses = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.Barracks)\n",
    "        marines = self.get_my_units_by_type(obs, units.Terran.Marine)\n",
    "\n",
    "        queued_marines = (completed_barrackses[0].order_length\n",
    "        if len(completed_barrackses) > 0 else 0)\n",
    "\n",
    "        free_supply = (obs.observation.player.food_cap -\n",
    "                       obs.observation.player.food_used)\n",
    "        can_afford_supply_depot = obs.observation.player.minerals >= 100\n",
    "        can_afford_barracks = obs.observation.player.minerals >= 150\n",
    "        can_afford_marine = obs.observation.player.minerals >= 100\n",
    "\n",
    "        enemy_scvs = self.get_enemy_units_by_type(obs, units.Terran.SCV)\n",
    "        enemy_idle_scvs = [scv for scv in enemy_scvs if scv.order_length == 0]\n",
    "        enemy_command_centers = self.get_enemy_units_by_type(\n",
    "            obs, units.Terran.CommandCenter)\n",
    "        enemy_supply_depots = self.get_enemy_units_by_type(\n",
    "            obs, units.Terran.SupplyDepot)\n",
    "        enemy_completed_supply_depots = self.get_enemy_completed_units_by_type(\n",
    "            obs, units.Terran.SupplyDepot)\n",
    "        enemy_barrackses = self.get_enemy_units_by_type(obs, units.Terran.Barracks)\n",
    "        enemy_completed_barrackses = self.get_enemy_completed_units_by_type(\n",
    "            obs, units.Terran.Barracks)\n",
    "        enemy_marines = self.get_enemy_units_by_type(obs, units.Terran.Marine)\n",
    "\n",
    "        return (len(command_centers),\n",
    "                len(scvs),\n",
    "                len(idle_scvs),\n",
    "                len(supply_depots),\n",
    "                len(completed_supply_depots),\n",
    "                len(barrackses),\n",
    "                len(completed_barrackses),\n",
    "                len(marines),\n",
    "                queued_marines,\n",
    "                free_supply,\n",
    "                can_afford_supply_depot,\n",
    "                can_afford_barracks,\n",
    "                can_afford_marine,\n",
    "                len(enemy_command_centers),\n",
    "                len(enemy_scvs),\n",
    "                len(enemy_idle_scvs),\n",
    "                len(enemy_supply_depots),\n",
    "                len(enemy_completed_supply_depots),\n",
    "                len(enemy_barrackses),\n",
    "                len(enemy_completed_barrackses),\n",
    "                len(enemy_marines))\n",
    "\n",
    "    def step(self, obs):\n",
    "        super(TerranRLAgentWithRawActsAndRawObs, self).step(obs)\n",
    "        \n",
    "        #time.sleep(0.5)\n",
    "        \n",
    "        state = str(self.get_state(obs))\n",
    "        action = self.qlearn.choose_action(state)\n",
    "        if self.previous_action is not None:\n",
    "            self.qlearn.learn(self.previous_state,\n",
    "                              self.previous_action,\n",
    "                              obs.reward,\n",
    "                              'terminal' if obs.last() else state)\n",
    "        self.previous_state = state\n",
    "        self.previous_action = action\n",
    "        \n",
    "        if obs.last():\n",
    "            self.qlearn.q_table.to_pickle(self.data_file + '.gz', 'gzip')\n",
    "\n",
    "        return getattr(self, action)(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [run code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Applying NaiveDQN to a PySC2 Agent\n",
    "\n",
    "- Implementing \"Neural Q-Learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import pickle\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from skdrl.pytorch.model.mlp import NaiveMultiLayerPerceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'rlagent_with_naive_dqn'\n",
    "SCORE_FILE = 'rlagent_with_naive_dqn_score'\n",
    "\n",
    "scores = []                        # list containing scores from each episode\n",
    "scores_window = deque(maxlen=100)  # last 100 scores\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class NaiveDQN(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 state_dim: int,\n",
    "                 action_dim: int,\n",
    "                 qnet: nn.Module,\n",
    "                 lr: float,\n",
    "                 gamma: float,\n",
    "                 epsilon: float):\n",
    "        super(NaiveDQN, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.qnet = qnet\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.opt = torch.optim.Adam(params=self.qnet.parameters(), lr=lr)\n",
    "        self.register_buffer('epsilon', torch.ones(1) * epsilon)\n",
    "\n",
    "        self.criteria = nn.MSELoss()\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        qs = self.qnet(state)  # Notice that qs is 2d tensor [batch x action]\n",
    "\n",
    "        if self.train:  # epsilon-greedy policy\n",
    "            #prob = np.random.uniform(0.0, 1.0, 1)\n",
    "            #if torch.from_numpy(prob).float() <= self.epsilon:  # random\n",
    "            if random.random() <= self.epsilon: # random\n",
    "                action = np.random.choice(range(self.action_dim))\n",
    "            else:  # greedy\n",
    "                action = qs.argmax(dim=-1)\n",
    "        else:  # greedy policy\n",
    "            action = qs.argmax(dim=-1)\n",
    "        return int(action)\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        s, a, r, ns = state, action, reward, next_state\n",
    "        # Q-Learning target\n",
    "        q_max, _ = self.qnet(ns).max(dim=-1)\n",
    "        q_target = r + self.gamma * q_max * (1 - done)\n",
    "\n",
    "        # Don't forget to detach `td_target` from the computational graph\n",
    "        q_target = q_target.detach()\n",
    "\n",
    "        # Or you can follow a better practice as follows:\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            q_max, _ = self.qnet(ns).max(dim=-1)\n",
    "            q_target = r + self.gamma * q_max * (1 - done)\n",
    "        \"\"\"\n",
    "\n",
    "        loss = self.criteria(self.qnet(s)[0, a], q_target)\n",
    "        self.opt.zero_grad()\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranAgentWithRawActsAndRawObs(base_agent.BaseAgent):\n",
    "    actions = (\"do_nothing\",\n",
    "               \"harvest_minerals\",\n",
    "               \"build_supply_depot\",\n",
    "               \"build_barracks\",\n",
    "               \"train_marine\",\n",
    "               \"attack\")\n",
    "\n",
    "    def get_my_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.raw_units\n",
    "                if unit.unit_type == unit_type\n",
    "                and unit.alliance == features.PlayerRelative.SELF]\n",
    "\n",
    "    def get_enemy_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.raw_units\n",
    "                if unit.unit_type == unit_type\n",
    "                and unit.alliance == features.PlayerRelative.ENEMY]\n",
    "\n",
    "    def get_my_completed_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.raw_units\n",
    "                if unit.unit_type == unit_type\n",
    "                and unit.build_progress == 100\n",
    "                and unit.alliance == features.PlayerRelative.SELF]\n",
    "\n",
    "    def get_enemy_completed_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.raw_units\n",
    "                if unit.unit_type == unit_type\n",
    "                and unit.build_progress == 100\n",
    "                and unit.alliance == features.PlayerRelative.ENEMY]\n",
    "\n",
    "    def get_distances(self, obs, units, xy):\n",
    "        units_xy = [(unit.x, unit.y) for unit in units]\n",
    "        return np.linalg.norm(np.array(units_xy) - np.array(xy), axis=1)\n",
    "\n",
    "    def step(self, obs):\n",
    "        super(TerranAgentWithRawActsAndRawObs, self).step(obs)\n",
    "        if obs.first():\n",
    "            command_center = self.get_my_units_by_type(\n",
    "                obs, units.Terran.CommandCenter)[0]\n",
    "            self.base_top_left = (command_center.x < 32)\n",
    "\n",
    "    def do_nothing(self, obs):\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "    def harvest_minerals(self, obs):\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        idle_scvs = [scv for scv in scvs if scv.order_length == 0]\n",
    "        if len(idle_scvs) > 0:\n",
    "            mineral_patches = [unit for unit in obs.observation.raw_units\n",
    "                               if unit.unit_type in [\n",
    "                                   units.Neutral.BattleStationMineralField,\n",
    "                                   units.Neutral.BattleStationMineralField750,\n",
    "                                   units.Neutral.LabMineralField,\n",
    "                                   units.Neutral.LabMineralField750,\n",
    "                                   units.Neutral.MineralField,\n",
    "                                   units.Neutral.MineralField750,\n",
    "                                   units.Neutral.PurifierMineralField,\n",
    "                                   units.Neutral.PurifierMineralField750,\n",
    "                                   units.Neutral.PurifierRichMineralField,\n",
    "                                   units.Neutral.PurifierRichMineralField750,\n",
    "                                   units.Neutral.RichMineralField,\n",
    "                                   units.Neutral.RichMineralField750\n",
    "                               ]]\n",
    "            scv = random.choice(idle_scvs)\n",
    "            distances = self.get_distances(obs, mineral_patches, (scv.x, scv.y))\n",
    "            mineral_patch = mineral_patches[np.argmin(distances)]\n",
    "            return actions.RAW_FUNCTIONS.Harvest_Gather_unit(\n",
    "                \"now\", scv.tag, mineral_patch.tag)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "    def build_supply_depot(self, obs):\n",
    "        supply_depots = self.get_my_units_by_type(obs, units.Terran.SupplyDepot)\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        if (len(supply_depots) == 0 and obs.observation.player.minerals >= 100 and\n",
    "                len(scvs) > 0):\n",
    "            supply_depot_xy = (22, 26) if self.base_top_left else (35, 42)\n",
    "            distances = self.get_distances(obs, scvs, supply_depot_xy)\n",
    "            scv = scvs[np.argmin(distances)]\n",
    "            return actions.RAW_FUNCTIONS.Build_SupplyDepot_pt(\n",
    "                \"now\", scv.tag, supply_depot_xy)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "    def build_barracks(self, obs):\n",
    "        completed_supply_depots = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.SupplyDepot)\n",
    "        barrackses = self.get_my_units_by_type(obs, units.Terran.Barracks)\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        if (len(completed_supply_depots) > 0 and len(barrackses) == 0 and\n",
    "                obs.observation.player.minerals >= 150 and len(scvs) > 0):\n",
    "            barracks_xy = (22, 21) if self.base_top_left else (35, 45)\n",
    "            distances = self.get_distances(obs, scvs, barracks_xy)\n",
    "            scv = scvs[np.argmin(distances)]\n",
    "            return actions.RAW_FUNCTIONS.Build_Barracks_pt(\n",
    "                \"now\", scv.tag, barracks_xy)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "    def train_marine(self, obs):\n",
    "        completed_barrackses = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.Barracks)\n",
    "        free_supply = (obs.observation.player.food_cap -\n",
    "                       obs.observation.player.food_used)\n",
    "        if (len(completed_barrackses) > 0 and obs.observation.player.minerals >= 100\n",
    "                and free_supply > 0):\n",
    "            barracks = self.get_my_units_by_type(obs, units.Terran.Barracks)[0]\n",
    "            if barracks.order_length < 5:\n",
    "                return actions.RAW_FUNCTIONS.Train_Marine_quick(\"now\", barracks.tag)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "    def attack(self, obs):\n",
    "        marines = self.get_my_units_by_type(obs, units.Terran.Marine)\n",
    "        if len(marines) > 0:\n",
    "            attack_xy = (38, 44) if self.base_top_left else (19, 23)\n",
    "            distances = self.get_distances(obs, marines, attack_xy)\n",
    "            marine = marines[np.argmax(distances)]\n",
    "            x_offset = random.randint(-4, 4)\n",
    "            y_offset = random.randint(-4, 4)\n",
    "            return actions.RAW_FUNCTIONS.Attack_pt(\n",
    "                \"now\", marine.tag, (attack_xy[0] + x_offset, attack_xy[1] + y_offset))\n",
    "        return actions.RAW_FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranRandomAgent(TerranAgentWithRawActsAndRawObs):\n",
    "    def step(self, obs):\n",
    "        super(TerranRandomAgent, self).step(obs)\n",
    "        action = random.choice(self.actions)\n",
    "        return getattr(self, action)(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s_dim = 21\n",
    "a_dim = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DRL 모델의 성능 추이를 보기위해 Reward의 평균 추이를 이용한다. 이때 단순이동평균 보다는 지수이동평균이 적절하다.\n",
    "\n",
    "### 지수이동평균(EMA:Exponential Moving Average) 란?\n",
    "지수이동평균(Exponential Moving Average)은 과거의 모든 기간을 계산대상으로 하며 최근의 데이타에 더 높은 가중치를 두는 일종의 가중이동평균법이다.\n",
    "\n",
    "단순이동평균의 계산법에 비하여 원리가 복잡해 보이지만 실제로 이동평균을 산출하는 방법은 Previous Step의 지수이동평균값과 평활계수(smoothing constant) 그리고 당일의 가격만으로 구할 수 있으므로 Previous Step의 지수이동평균값만 구해진다면 오히려 간단한 편이다.\n",
    "\n",
    "따라서 지수이동평균은 단순이동평균에 비해 몇가지 중요한 강점을 가진다.\n",
    "\n",
    "첫째는 가장 최근의 Step에 가장 큰 가중치를 둠으로 해서 최근의 Episode들을 잘 반영한다는 점이고, 둘째는 단순이동평균에서와 같이 오래된 데이타를 갑자기 제외하지 않고 천천히 그 영향력을 사라지게 한다는 점이다.\n",
    "또한 전 기간의 데이타를 분석대상으로 함으로써 가중이동평균에서 문제되는 특정 기간의 데이타만을 분석대상으로 한다는 단점도 보완하고 있다.\n",
    "\n",
    "### 지수이동평균(EMA:Exponential Moving Average) 계산\n",
    "\n",
    "지수이동평균은 가장 최근의 값에 많은 가중치를 부여하고 오래 된 값에는 적은 가중치를 부여한다. 비록 오래 된 값이라고 할지라도 완전히 무시하지는 않고 적게나마 반영시켜 계산한다는 장점이 있다. 단기 변동성을 포착하려는 것이 목적이다.\n",
    "\n",
    "EMA=Previous Step 지수이동평균+(k∗(Current Step Reward − Previous Step 지수이동평균))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMAMeter:\n",
    "\n",
    "    def __init__(self,\n",
    "                 alpha: float = 0.5):\n",
    "        self.s = None\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def update(self, y):\n",
    "        if self.s is None:\n",
    "            self.s = y\n",
    "        else:\n",
    "            self.s = self.alpha * y + (1 - self.alpha) * self.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranRLAgentWithRawActsAndRawObs(TerranAgentWithRawActsAndRawObs):\n",
    "    def __init__(self):\n",
    "        super(TerranRLAgentWithRawActsAndRawObs, self).__init__()\n",
    "\n",
    "        self.s_dim = 21\n",
    "        self.a_dim = 6\n",
    "        self.lr = 1e-4\n",
    "        self.gamma = 1.0\n",
    "        self.epsilon = 1.0\n",
    "\n",
    "        self.qnetwork = NaiveMultiLayerPerceptron(input_dim=self.s_dim,\n",
    "                           output_dim=self.a_dim,\n",
    "                           num_neurons=[128],\n",
    "                           hidden_act_func='ReLU',\n",
    "                           out_act_func='Identity').to(device)\n",
    "\n",
    "        self.data_file = DATA_FILE\n",
    "        if os.path.isfile(self.data_file + '.pt'):\n",
    "            self.qnetwork.load_state_dict(torch.load(self.data_file + '.pt'))\n",
    "\n",
    "        self.dqn = NaiveDQN(state_dim=self.s_dim,\n",
    "                             action_dim=self.a_dim,\n",
    "                             qnet=self.qnetwork,\n",
    "                             lr=self.lr,\n",
    "                             gamma=self.gamma,\n",
    "                             epsilon=self.epsilon).to(device)\n",
    "\n",
    "        self.score_file = SCORE_FILE\n",
    "        \n",
    "        self.print_every = 50\n",
    "        self.ema_factor = 0.5\n",
    "        self.ema = EMAMeter(self.ema_factor)\n",
    "        self.cum_reward = 0\n",
    "        self.cum_loss = 0\n",
    "        self.episode_count = 0\n",
    "\n",
    "        self.new_game()\n",
    "\n",
    "    def reset(self):\n",
    "        super(TerranRLAgentWithRawActsAndRawObs, self).reset()\n",
    "        self.new_game()\n",
    "\n",
    "    def new_game(self):\n",
    "        self.base_top_left = None\n",
    "        self.previous_state = None\n",
    "        self.previous_action = None\n",
    "        self.cum_reward = 0\n",
    "        self.cum_loss = 0\n",
    "\n",
    "    def get_state(self, obs):\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        idle_scvs = [scv for scv in scvs if scv.order_length == 0]\n",
    "        command_centers = self.get_my_units_by_type(obs, units.Terran.CommandCenter)\n",
    "        supply_depots = self.get_my_units_by_type(obs, units.Terran.SupplyDepot)\n",
    "        completed_supply_depots = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.SupplyDepot)\n",
    "        barrackses = self.get_my_units_by_type(obs, units.Terran.Barracks)\n",
    "        completed_barrackses = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.Barracks)\n",
    "        marines = self.get_my_units_by_type(obs, units.Terran.Marine)\n",
    "\n",
    "        queued_marines = (completed_barrackses[0].order_length\n",
    "        if len(completed_barrackses) > 0 else 0)\n",
    "\n",
    "        free_supply = (obs.observation.player.food_cap -\n",
    "                       obs.observation.player.food_used)\n",
    "        can_afford_supply_depot = obs.observation.player.minerals >= 100\n",
    "        can_afford_barracks = obs.observation.player.minerals >= 150\n",
    "        can_afford_marine = obs.observation.player.minerals >= 100\n",
    "\n",
    "        enemy_scvs = self.get_enemy_units_by_type(obs, units.Terran.SCV)\n",
    "        enemy_idle_scvs = [scv for scv in enemy_scvs if scv.order_length == 0]\n",
    "        enemy_command_centers = self.get_enemy_units_by_type(\n",
    "            obs, units.Terran.CommandCenter)\n",
    "        enemy_supply_depots = self.get_enemy_units_by_type(\n",
    "            obs, units.Terran.SupplyDepot)\n",
    "        enemy_completed_supply_depots = self.get_enemy_completed_units_by_type(\n",
    "            obs, units.Terran.SupplyDepot)\n",
    "        enemy_barrackses = self.get_enemy_units_by_type(obs, units.Terran.Barracks)\n",
    "        enemy_completed_barrackses = self.get_enemy_completed_units_by_type(\n",
    "            obs, units.Terran.Barracks)\n",
    "        enemy_marines = self.get_enemy_units_by_type(obs, units.Terran.Marine)\n",
    "\n",
    "        return (len(command_centers),\n",
    "                len(scvs),\n",
    "                len(idle_scvs),\n",
    "                len(supply_depots),\n",
    "                len(completed_supply_depots),\n",
    "                len(barrackses),\n",
    "                len(completed_barrackses),\n",
    "                len(marines),\n",
    "                queued_marines,\n",
    "                free_supply,\n",
    "                can_afford_supply_depot,\n",
    "                can_afford_barracks,\n",
    "                can_afford_marine,\n",
    "                len(enemy_command_centers),\n",
    "                len(enemy_scvs),\n",
    "                len(enemy_idle_scvs),\n",
    "                len(enemy_supply_depots),\n",
    "                len(enemy_completed_supply_depots),\n",
    "                len(enemy_barrackses),\n",
    "                len(enemy_completed_barrackses),\n",
    "                len(enemy_marines))\n",
    "\n",
    "    def step(self, obs):\n",
    "        super(TerranRLAgentWithRawActsAndRawObs, self).step(obs)\n",
    "\n",
    "        #time.sleep(0.5)\n",
    "\n",
    "        state = self.get_state(obs)\n",
    "        state = torch.tensor(state).float().view(1, self.s_dim).to(device)\n",
    "        action_idx = self.dqn.choose_action(state)\n",
    "        action = self.actions[action_idx]\n",
    "        done = True if obs.last() else False\n",
    "        if self.previous_action is not None:\n",
    "            loss = self.dqn.learn(self.previous_state.to(device),\n",
    "                              torch.tensor(self.previous_action).view(1, 1).to(device),\n",
    "                              torch.tensor(obs.reward).view(1, 1).to(device),\n",
    "                              state.to(device),\n",
    "                              torch.tensor(done).float().view(1, 1).to(device)\n",
    "                              )\n",
    "            self.cum_loss += loss.detach().numpy()\n",
    "        self.cum_reward += obs.reward\n",
    "        self.previous_state = state\n",
    "        self.previous_action = action_idx\n",
    "\n",
    "        if obs.last():\n",
    "            self.episode_count = self.episode_count + 1\n",
    "            torch.save(self.dqn.qnet.state_dict(), self.data_file + '.pt')\n",
    "\n",
    "            scores_window.append(obs.reward)  # save most recent reward\n",
    "            win_rate = scores_window.count(1)/len(scores_window)*100\n",
    "            tie_rate = scores_window.count(0)/len(scores_window)*100\n",
    "            lost_rate = scores_window.count(-1)/len(scores_window)*100\n",
    "            \n",
    "            scores.append([win_rate, tie_rate, lost_rate])  # save most recent score(win_rate, tie_rate, lost_rate)\n",
    "            with open(self.score_file + '.txt', \"wb\") as fp:\n",
    "                pickle.dump(scores, fp)\n",
    "            \n",
    "            self.ema.update(self.cum_reward)\n",
    "            writer.add_scalar(\"Loss/online\", self.cum_loss/obs.observation.game_loop, self.episode_count)\n",
    "            writer.add_scalar(\"Score\", self.ema.s, self.episode_count)\n",
    "\n",
    "            if self.episode_count % self.print_every == 0:\n",
    "                print(\"Episode {} || EMA: {} || EPS : {}\".format(self.episode_count, self.ema.s, self.dqn.epsilon))\n",
    "\n",
    "            if self.episode_count >= 150:\n",
    "                self.dqn.epsilon *= 0.999\n",
    "\n",
    "        return getattr(self, action)(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [run code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Winning rate graph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SCORE_FILE + '.txt', \"rb\") as fp:\n",
    "    scores = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_scores = np.array(scores)\n",
    "np_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(np_scores)), np_scores.T[0], color='r', label='win rate')\n",
    "plt.plot(np.arange(len(np_scores)), np_scores.T[1], color='g', label='tie rate')\n",
    "plt.plot(np.arange(len(np_scores)), np_scores.T[2], color='b', label='lose rate')\n",
    "plt.ylabel('Score %')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Applying Vanilla DQN to a PySC2 Agent\n",
    "\n",
    "구현된 기능\n",
    "\n",
    "- Implementing 'Experience Replay' : \n",
    "    - 'Maximization Bias' 문제를 발생시키는 원인 중 하나인 'Sample간의 시간적 연관성'을 해결하기 위한 방법\n",
    "    - Online Learning 에서 Batch Learning 으로 학습방법 바뀜 : Online update 는 Batch update 보다 일반적으로 Validation loss 가 더 높게 나타남.\n",
    "    - Reinforcement Learning for Robots. Using Neural Networks. Long -Ji Lin. January 6, 1993. 논문에서 최초로 연구됨 http://isl.anthropomatik.kit.edu/pdf/Lin1993.pdf\n",
    "\n",
    "- Implementing 'Fixed Q-Target' : \n",
    "    - 'Moving Q-Target' 문제 해결하기 위한 방법\n",
    "    - 2015년 Nature 버전 DQN 논문에서 처음 제안됨. https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning \n",
    "\n",
    "\n",
    "구현되지 않은 기능\n",
    "\n",
    "- Implementing 'Sensory Input Feature-Extraction' :\n",
    "    - 게임의 Raw Image 를 Neural Net에 넣기 위한 Preprocessing(전처리) 과정\n",
    "    - Raw Image 의 Sequence중 '최근 4개의 이미지'(과거 정보)를 하나의 새로운 State로 정의하여 non-MDP를 MDP 문제로 바꾸는 Preprocessing 과정 \n",
    "    - CNN(합성곱 신경망)을 활용한 '차원의 저주' 극복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import pickle\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from skdrl.pytorch.model.mlp import NaiveMultiLayerPerceptron\n",
    "from skdrl.pytorch.model.cnn import CNNFC\n",
    "from skdrl.common.memory.memory import ExperienceReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE_QNET = 'rlagent_with_vanilla_dqn_qnet'\n",
    "DATA_FILE_QNET_TARGET = 'rlagent_with_vanilla_dqn_qnet_target'\n",
    "SCORE_FILE = 'rlagent_with_vanilla_dqn_score'\n",
    "\n",
    "scores = []                        # list containing scores from each episode\n",
    "scores_window = deque(maxlen=100)  # last 100 scores\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-update 공식\n",
    "\n",
    "#### 1. Online Q-learning\n",
    "![Online Q-learning](./images/q-update-experience-replay.png)\n",
    "\n",
    "#### 2. Online Q-learning with Function Approximation\n",
    "![Online Q-learning with Function Approximation](./images/q-update-function-approximation.png)\n",
    "\n",
    "#### 3. Batch Q-learning with Function Approximation & Experience Replay\n",
    "![Batch Q-learning with Function Approximation & Experience Replay](./images/q-update-online.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving target problem\n",
    "\n",
    "#### 1. Function Approximation을 사용하지 않는 Q-learning 의 경우 : 특정한 Q(s,a) update가 다른 Q(s,a)에 영향을 주지 않는다.\n",
    "![Moving target Q-learning](./images/moving-target_q-learing_case.png)\n",
    "\n",
    "#### 2. Function Approximation을 사용하는 Q-learnig 의 경우 : 특정한 Q(s,a) update가 다른 Q(s,a)에 영향을 준다.\n",
    "![Moving target Q-learning with Function Approximation](./images/moving-target_q-learing_with_function_approximation_case.png)\n",
    "\n",
    "### Moving target 문제는 Deep Neural Network를 사용하는 Function Approximation 기법인 경우 심해지는 경향성이 있음.\n",
    "\n",
    "image ref : Fast Campus RL online courese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.SmoothL1Loss()` = Huber loss 란?\n",
    "\n",
    "Mean-squared Error (MSE) Loss 는 데이터의 outlier에 매우 취약하다.\n",
    "어떤 이유로 타겟하는 레이블 y (이 경우는 q-learning target)이 noisy 할때를 가정하면, 잘못된 y 값을 맞추기 위해 파라미터들이 너무 sensitive 하게 움직이게 된다.\n",
    "\n",
    "이런 현상은 q-learning 의 학습초기에 매우 빈번해 나타난다. 이러한 문제를 조금이라도 완화하기 위해서 outlier에 덜 민감한 Huber loss 함수를 사용한다.\n",
    "\n",
    "### SmoothL1Loss (aka Huber loss)\n",
    "\n",
    "$$loss(x,y) = \\frac{1}{n}\\sum_i z_i$$\n",
    "$|x_i - y_i| <1$ 일때,\n",
    "$$z_i = 0.5(x_i - y_i)^2$$\n",
    "$|x_i - y_i| \\geq1$ 일때,\n",
    "$$z_i = |x_i - y_i|-0.5$$\n",
    "\n",
    "ref : https://pytorch.org/docs/master/generated/torch.nn.SmoothL1Loss.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 state_dim: int,\n",
    "                 action_dim: int,\n",
    "                 qnet: nn.Module,\n",
    "                 qnet_target: nn.Module,\n",
    "                 lr: float,\n",
    "                 gamma: float,\n",
    "                 epsilon: float):\n",
    "        \"\"\"\n",
    "        :param state_dim: input state dimension\n",
    "        :param action_dim: action dimension\n",
    "        :param qnet: main q network\n",
    "        :param qnet_target: target q network\n",
    "        :param lr: learning rate\n",
    "        :param gamma: discount factor of MDP\n",
    "        :param epsilon: E-greedy factor\n",
    "        \"\"\"\n",
    "\n",
    "        super(DQN, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.qnet = qnet\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.opt = torch.optim.Adam(params=self.qnet.parameters(), lr=lr)\n",
    "        self.register_buffer('epsilon', torch.ones(1) * epsilon)\n",
    "\n",
    "        # target network related\n",
    "        qnet_target.load_state_dict(qnet.state_dict())\n",
    "        self.qnet_target = qnet_target\n",
    "        self.criteria = nn.SmoothL1Loss()\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        qs = self.qnet(state)\n",
    "        #prob = np.random.uniform(0.0, 1.0, 1)\n",
    "        #if torch.from_numpy(prob).float() <= self.epsilon:  # random\n",
    "        if random.random() <= self.epsilon: # random\n",
    "            action = np.random.choice(range(self.action_dim))\n",
    "        else:  # greedy\n",
    "            print(qs)\n",
    "            action = qs.argmax(dim=-1)\n",
    "            #action = torch.argmax(qs, dim=-1)\n",
    "        return int(action)\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        s, a, r, ns = state, action, reward, next_state\n",
    "\n",
    "        # compute Q-Learning target with 'target network'\n",
    "        with torch.no_grad():\n",
    "            q_max, _ = self.qnet_target(ns).max(dim=-1, keepdims=True)\n",
    "            q_target = r + self.gamma * q_max * (1 - done)\n",
    "\n",
    "        q_val = self.qnet(s).gather(1, a)\n",
    "        #print(q_val)\n",
    "        #print(q_target)\n",
    "        loss = self.criteria(q_val, q_target)\n",
    "        print('loss : {}'.format(loss))\n",
    "\n",
    "        self.opt.zero_grad()\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "\n",
    "\n",
    "def prepare_training_inputs(sampled_exps, device='cpu'):\n",
    "    map_screen = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    next_states = []\n",
    "    dones = []\n",
    "    for sampled_exp in sampled_exps:\n",
    "        states.append(sampled_exp[0])\n",
    "        actions.append(sampled_exp[1])\n",
    "        rewards.append(sampled_exp[2])\n",
    "        next_states.append(sampled_exp[3])\n",
    "        dones.append(sampled_exp[4])\n",
    "        #map_screen.append(sampled_exp[5])\n",
    "\n",
    "    states = torch.cat(states, dim=0).float().to(device)\n",
    "    actions = torch.cat(actions, dim=0).to(device)\n",
    "    rewards = torch.cat(rewards, dim=0).float().to(device)\n",
    "    next_states = torch.cat(next_states, dim=0).float().to(device)\n",
    "    dones = torch.cat(dones, dim=0).float().to(device)\n",
    "    #map_screen = torch.cat(map_screen, dim=0).float().to(device)\n",
    "    return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranAgentWithRawActsAndRawObs(base_agent.BaseAgent):\n",
    "    actions = (\"do_nothing\",\n",
    "               \"harvest_minerals\",\n",
    "               \"build_supply_depot\",\n",
    "               \"build_barracks\",\n",
    "               \"train_marine\",\n",
    "               \"attack\")\n",
    "\n",
    "    def get_my_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.raw_units\n",
    "                if unit.unit_type == unit_type\n",
    "                and unit.alliance == features.PlayerRelative.SELF]\n",
    "\n",
    "    def get_enemy_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.raw_units\n",
    "                if unit.unit_type == unit_type\n",
    "                and unit.alliance == features.PlayerRelative.ENEMY]\n",
    "\n",
    "    def get_my_completed_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.raw_units\n",
    "                if unit.unit_type == unit_type\n",
    "                and unit.build_progress == 100\n",
    "                and unit.alliance == features.PlayerRelative.SELF]\n",
    "\n",
    "    def get_enemy_completed_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.raw_units\n",
    "                if unit.unit_type == unit_type\n",
    "                and unit.build_progress == 100\n",
    "                and unit.alliance == features.PlayerRelative.ENEMY]\n",
    "\n",
    "    def get_distances(self, obs, units, xy):\n",
    "        units_xy = [(unit.x, unit.y) for unit in units]\n",
    "        return np.linalg.norm(np.array(units_xy) - np.array(xy), axis=1)\n",
    "\n",
    "    def step(self, obs):\n",
    "        super(TerranAgentWithRawActsAndRawObs, self).step(obs)\n",
    "        if obs.first():\n",
    "            command_center = self.get_my_units_by_type(\n",
    "                obs, units.Terran.CommandCenter)[0]\n",
    "            self.base_top_left = (command_center.x < 32)\n",
    "\n",
    "    def do_nothing(self, obs):\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "    def harvest_minerals(self, obs):\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        idle_scvs = [scv for scv in scvs if scv.order_length == 0]\n",
    "        if len(idle_scvs) > 0:\n",
    "            mineral_patches = [unit for unit in obs.observation.raw_units\n",
    "                               if unit.unit_type in [\n",
    "                                   units.Neutral.BattleStationMineralField,\n",
    "                                   units.Neutral.BattleStationMineralField750,\n",
    "                                   units.Neutral.LabMineralField,\n",
    "                                   units.Neutral.LabMineralField750,\n",
    "                                   units.Neutral.MineralField,\n",
    "                                   units.Neutral.MineralField750,\n",
    "                                   units.Neutral.PurifierMineralField,\n",
    "                                   units.Neutral.PurifierMineralField750,\n",
    "                                   units.Neutral.PurifierRichMineralField,\n",
    "                                   units.Neutral.PurifierRichMineralField750,\n",
    "                                   units.Neutral.RichMineralField,\n",
    "                                   units.Neutral.RichMineralField750\n",
    "                               ]]\n",
    "            scv = random.choice(idle_scvs)\n",
    "            distances = self.get_distances(obs, mineral_patches, (scv.x, scv.y))\n",
    "            mineral_patch = mineral_patches[np.argmin(distances)]\n",
    "            return actions.RAW_FUNCTIONS.Harvest_Gather_unit(\n",
    "                \"now\", scv.tag, mineral_patch.tag)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "    def build_supply_depot(self, obs):\n",
    "        supply_depots = self.get_my_units_by_type(obs, units.Terran.SupplyDepot)\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        if (len(supply_depots) == 0 and obs.observation.player.minerals >= 100 and\n",
    "                len(scvs) > 0):\n",
    "            supply_depot_xy = (22, 26) if self.base_top_left else (35, 42)\n",
    "            distances = self.get_distances(obs, scvs, supply_depot_xy)\n",
    "            scv = scvs[np.argmin(distances)]\n",
    "            return actions.RAW_FUNCTIONS.Build_SupplyDepot_pt(\n",
    "                \"now\", scv.tag, supply_depot_xy)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "    def build_barracks(self, obs):\n",
    "        completed_supply_depots = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.SupplyDepot)\n",
    "        barrackses = self.get_my_units_by_type(obs, units.Terran.Barracks)\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        if (len(completed_supply_depots) > 0 and len(barrackses) == 0 and\n",
    "                obs.observation.player.minerals >= 150 and len(scvs) > 0):\n",
    "            barracks_xy = (22, 21) if self.base_top_left else (35, 45)\n",
    "            distances = self.get_distances(obs, scvs, barracks_xy)\n",
    "            scv = scvs[np.argmin(distances)]\n",
    "            return actions.RAW_FUNCTIONS.Build_Barracks_pt(\n",
    "                \"now\", scv.tag, barracks_xy)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "    def train_marine(self, obs):\n",
    "        completed_barrackses = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.Barracks)\n",
    "        free_supply = (obs.observation.player.food_cap -\n",
    "                       obs.observation.player.food_used)\n",
    "        if (len(completed_barrackses) > 0 and obs.observation.player.minerals >= 100\n",
    "                and free_supply > 0):\n",
    "            barracks = self.get_my_units_by_type(obs, units.Terran.Barracks)[0]\n",
    "            if barracks.order_length < 5:\n",
    "                return actions.RAW_FUNCTIONS.Train_Marine_quick(\"now\", barracks.tag)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "    def attack(self, obs):\n",
    "        marines = self.get_my_units_by_type(obs, units.Terran.Marine)\n",
    "        if len(marines) > 0:\n",
    "            attack_xy = (38, 44) if self.base_top_left else (19, 23)\n",
    "            distances = self.get_distances(obs, marines, attack_xy)\n",
    "            marine = marines[np.argmax(distances)]\n",
    "            x_offset = random.randint(-4, 4)\n",
    "            y_offset = random.randint(-4, 4)\n",
    "            return actions.RAW_FUNCTIONS.Attack_pt(\n",
    "                \"now\", marine.tag, (attack_xy[0] + x_offset, attack_xy[1] + y_offset))\n",
    "        return actions.RAW_FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranRandomAgent(TerranAgentWithRawActsAndRawObs):\n",
    "    def step(self, obs):\n",
    "        super(TerranRandomAgent, self).step(obs)\n",
    "        action = random.choice(self.actions)\n",
    "        return getattr(self, action)(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter\n",
    "\n",
    "하이퍼파라미터는 심층강화학습 알고리즘에서 성능에 매우 큰 영향을 미칩니다.\n",
    "이 실험에 쓰인 하이퍼파라미터는 https://github.com/chucnorrisful/dqn 실험에서 제안된 값들을 참고하였습니다.\n",
    "\n",
    "\n",
    "- self.s_dim = 21\n",
    "- self.a_dim = 6\n",
    "\n",
    "- self.lr = 1e-4 * 1\n",
    "- self.batch_size = 32\n",
    "- self.gamma = 0.99\n",
    "- self.memory_size = 200000\n",
    "- self.eps_max = 1.0\n",
    "- self.eps_min = 0.01\n",
    "- self.epsilon = 1.0\n",
    "- self.init_sampling = 4000\n",
    "- self.target_update_interval = 10\n",
    "\n",
    "- self.epsilon = max(self.eps_min, self.eps_max - self.eps_min * (self.episode_count / 50))\n",
    "\n",
    "\n",
    "![Winning rate graph](./images/rlagent_with_vanilla_dqn_score-Terran-Terran-495_Eps.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranRLAgentWithRawActsAndRawObs(TerranAgentWithRawActsAndRawObs):\n",
    "    def __init__(self):\n",
    "        super(TerranRLAgentWithRawActsAndRawObs, self).__init__()\n",
    "\n",
    "        self.s_dim = 21\n",
    "        self.a_dim = 6\n",
    "        \n",
    "        self.lr = 1e-4 * 1\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.99\n",
    "        self.memory_size = 200000\n",
    "        self.eps_max = 1.0\n",
    "        self.eps_min = 0.01\n",
    "        self.epsilon = 1.0\n",
    "        self.init_sampling = 4000\n",
    "        self.target_update_interval = 10\n",
    "\n",
    "        self.data_file_qnet = DATA_FILE_QNET\n",
    "        self.data_file_qnet_target = DATA_FILE_QNET_TARGET\n",
    "        self.score_file = SCORE_FILE\n",
    "        \n",
    "        \n",
    "        self.qnetwork = NaiveMultiLayerPerceptron(input_dim=(self.s_dim + 32*32 +32*32),\n",
    "                           output_dim=self.a_dim,\n",
    "                           num_neurons=[128, 64, 32],\n",
    "                           hidden_act_func='ReLU',\n",
    "                           out_act_func='Identity').to(device)\n",
    "        \n",
    "        self.qnetwork_target = NaiveMultiLayerPerceptron(input_dim=(self.s_dim + 32*32 + 32*32),\n",
    "                           output_dim=self.a_dim,\n",
    "                           num_neurons=[128, 64, 32],\n",
    "                           hidden_act_func='ReLU',\n",
    "                           out_act_func='Identity').to(device)\n",
    "        \n",
    "        '''\n",
    "        self.qnetwork = CNNFC(output_dim=self.a_dim).to(device)\n",
    "        \n",
    "        self.qnetwork_target = CNNFC(output_dim=self.a_dim).to(device)\n",
    "        '''\n",
    "        if os.path.isfile(self.data_file_qnet + '.pt'):\n",
    "            self.qnetwork.load_state_dict(torch.load(self.data_file_qnet + '.pt'))\n",
    "            \n",
    "        if os.path.isfile(self.data_file_qnet_target + '.pt'):\n",
    "            self.qnetwork_target.load_state_dict(torch.load(self.data_file_qnet_target + '.pt'))\n",
    "        \n",
    "        # initialize target network same as the main network.\n",
    "        self.qnetwork_target.load_state_dict(self.qnetwork.state_dict())\n",
    "\n",
    "        self.dqn = DQN(state_dim=self.s_dim,\n",
    "                             action_dim=self.a_dim,\n",
    "                             qnet=self.qnetwork,\n",
    "                             qnet_target=self.qnetwork_target,\n",
    "                             lr=self.lr,\n",
    "                             gamma=self.gamma,\n",
    "                             epsilon=self.epsilon).to(device)\n",
    "        \n",
    "        self.memory = ExperienceReplayMemory(self.memory_size)\n",
    "        \n",
    "        self.print_every = 1\n",
    "        self.cum_reward = 0\n",
    "        self.cum_loss = 0\n",
    "        self.episode_count = 0\n",
    "        \n",
    "        self.new_game()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        super(TerranRLAgentWithRawActsAndRawObs, self).reset()\n",
    "        self.new_game()\n",
    "\n",
    "    def new_game(self):\n",
    "        self.base_top_left = None\n",
    "        self.previous_state = None\n",
    "        self.previous_action = None\n",
    "        self.cum_reward = 0\n",
    "        self.cum_loss = 0\n",
    "        \n",
    "        # epsilon scheduling\n",
    "        # slowly decaying_epsilon\n",
    "        self.epsilon = max(self.eps_min, self.eps_max - self.eps_min * (self.episode_count / 50))\n",
    "        self.dqn.epsilon = torch.tensor(self.epsilon).to(device)\n",
    "        \n",
    "\n",
    "    def get_state(self, obs):\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        idle_scvs = [scv for scv in scvs if scv.order_length == 0]\n",
    "        command_centers = self.get_my_units_by_type(obs, units.Terran.CommandCenter)\n",
    "        supply_depots = self.get_my_units_by_type(obs, units.Terran.SupplyDepot)\n",
    "        completed_supply_depots = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.SupplyDepot)\n",
    "        barrackses = self.get_my_units_by_type(obs, units.Terran.Barracks)\n",
    "        completed_barrackses = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.Barracks)\n",
    "        marines = self.get_my_units_by_type(obs, units.Terran.Marine)\n",
    "\n",
    "        queued_marines = (completed_barrackses[0].order_length\n",
    "        if len(completed_barrackses) > 0 else 0)\n",
    "\n",
    "        free_supply = (obs.observation.player.food_cap -\n",
    "                       obs.observation.player.food_used)\n",
    "        can_afford_supply_depot = obs.observation.player.minerals >= 100\n",
    "        can_afford_barracks = obs.observation.player.minerals >= 150\n",
    "        can_afford_marine = obs.observation.player.minerals >= 100\n",
    "\n",
    "        enemy_scvs = self.get_enemy_units_by_type(obs, units.Terran.SCV)\n",
    "        enemy_idle_scvs = [scv for scv in enemy_scvs if scv.order_length == 0]\n",
    "        enemy_command_centers = self.get_enemy_units_by_type(\n",
    "            obs, units.Terran.CommandCenter)\n",
    "        enemy_supply_depots = self.get_enemy_units_by_type(\n",
    "            obs, units.Terran.SupplyDepot)\n",
    "        enemy_completed_supply_depots = self.get_enemy_completed_units_by_type(\n",
    "            obs, units.Terran.SupplyDepot)\n",
    "        enemy_barrackses = self.get_enemy_units_by_type(obs, units.Terran.Barracks)\n",
    "        enemy_completed_barrackses = self.get_enemy_completed_units_by_type(\n",
    "            obs, units.Terran.Barracks)\n",
    "        enemy_marines = self.get_enemy_units_by_type(obs, units.Terran.Marine)\n",
    "\n",
    "        return (len(command_centers),\n",
    "                len(scvs),\n",
    "                len(idle_scvs),\n",
    "                len(supply_depots),\n",
    "                len(completed_supply_depots),\n",
    "                len(barrackses),\n",
    "                len(completed_barrackses),\n",
    "                len(marines),\n",
    "                queued_marines,\n",
    "                free_supply,\n",
    "                can_afford_supply_depot,\n",
    "                can_afford_barracks,\n",
    "                can_afford_marine,\n",
    "                len(enemy_command_centers),\n",
    "                len(enemy_scvs),\n",
    "                len(enemy_idle_scvs),\n",
    "                len(enemy_supply_depots),\n",
    "                len(enemy_completed_supply_depots),\n",
    "                len(enemy_barrackses),\n",
    "                len(enemy_completed_barrackses),\n",
    "                len(enemy_marines))\n",
    "    \n",
    "    def get_minimap(self, obs):\n",
    "        minimap = np.array(obs.observation.feature_minimap[features.MINIMAP_FEATURES.player_relative.index], dtype=np.float32)\n",
    "        #minimap = np.array(obs.observation.feature_minimap, dtype=np.float32)\n",
    "        # [32, 32]\n",
    "        #print(minimap.shape)\n",
    "        return minimap\n",
    "\n",
    "    def get_screen(self, obs):\n",
    "        screen = np.array(obs.observation.feature_screen[features.SCREEN_FEATURES.player_relative.index], dtype=np.float32)\n",
    "        #screen = np.array(obs.observation.feature_screen, dtype=np.float32)\n",
    "        # [32, 32]\n",
    "        #print(screen.shape)\n",
    "        return screen\n",
    "    \n",
    "    def step(self, obs):\n",
    "        super(TerranRLAgentWithRawActsAndRawObs, self).step(obs)\n",
    "        \n",
    "        #time.sleep(0.5)\n",
    "        minimap = self.get_minimap(obs)\n",
    "        #minimap = np.reshape(minimap, (1, 32, 32))\n",
    "        minimap = np.reshape(minimap, (1, -1))\n",
    "        screen = self.get_screen(obs)\n",
    "        #screen = np.reshape(screen, (1, 32, 32))\n",
    "        screen = np.reshape(screen, (1, -1))\n",
    "        #screen = torch.tensor(screen).float().to(device)\n",
    "        #print(screen.shape)\n",
    "        map_screen = np.concatenate((minimap, screen), axis=-1)\n",
    "        #map_screen = np.reshape(map_screen, (1, 2, 32, 32))\n",
    "        map_screen = torch.tensor(map_screen).float().to(device)\n",
    "        #print(map_screen.shape)\n",
    "        state = self.get_state(obs)\n",
    "        #print(torch.tensor(state))\n",
    "        state = torch.tensor(state).float().view(1, 21).to(device)\n",
    "        state = torch.cat((map_screen, state), -1)\n",
    "        #print(state.shape)\n",
    "        # state = [1, ]\n",
    "        action_idx = self.dqn.choose_action(state)\n",
    "        #action_idx = self.dqn.choose_action(map_screen, state)\n",
    "        action = self.actions[action_idx]\n",
    "        done = True if obs.last() else False\n",
    "\n",
    "        if self.previous_action is not None:\n",
    "            experience = (self.previous_state.to(device),\n",
    "                          torch.tensor(self.previous_action).view(1, 1).to(device),\n",
    "                          torch.tensor(obs.reward).view(1, 1).to(device),\n",
    "                          state.to(device),\n",
    "                          torch.tensor(done).view(1, 1).to(device),\n",
    "                          #map_screen.to(device))\n",
    "                         )\n",
    "            self.memory.push(experience)\n",
    "        \n",
    "        self.cum_reward += obs.reward\n",
    "        self.previous_state = state\n",
    "        #self.previous_state = map_screen\n",
    "        self.previous_action = action_idx\n",
    "        \n",
    "        if obs.last():\n",
    "            self.episode_count = self.episode_count + 1\n",
    "            \n",
    "            if len(self.memory) >= self.init_sampling:\n",
    "                # training dqn\n",
    "                sampled_exps = self.memory.sample(self.batch_size)\n",
    "                sampled_exps = prepare_training_inputs(sampled_exps, device)\n",
    "                self.dqn.learn(*sampled_exps)\n",
    "\n",
    "            if self.episode_count % self.target_update_interval == 0:\n",
    "                self.dqn.qnet_target.load_state_dict(self.dqn.qnet.state_dict())\n",
    "\n",
    "            if self.episode_count % self.print_every == 0:\n",
    "                msg = (self.episode_count, self.cum_reward, self.epsilon)\n",
    "                print(\"Episode : {:4.0f} | Cumulative Reward : {:4.0f} | Epsilon : {:.3f}\".format(*msg))\n",
    "            \n",
    "            torch.save(self.dqn.qnet.state_dict(), self.data_file_qnet + '.pt')\n",
    "            torch.save(self.dqn.qnet_target.state_dict(), self.data_file_qnet_target + '.pt')\n",
    "\n",
    "            scores_window.append(obs.reward)  # save most recent reward\n",
    "            win_rate = scores_window.count(1)/len(scores_window)*100\n",
    "            tie_rate = scores_window.count(0)/len(scores_window)*100\n",
    "            lost_rate = scores_window.count(-1)/len(scores_window)*100\n",
    "            \n",
    "            scores.append([win_rate, tie_rate, lost_rate])  # save most recent score(win_rate, tie_rate, lost_rate)\n",
    "            with open(self.score_file + '.txt', \"wb\") as fp:\n",
    "                pickle.dump(scores, fp)\n",
    "            \n",
    "            #writer.add_scalar(\"Loss/train\", self.cum_lossobs.observation.game_loop, self.episode_count)\n",
    "            writer.add_scalar(\"Score\", self.cum_reward, self.episode_count)\n",
    "\n",
    "        return getattr(self, action)(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:08:28.893982 140235913836352 sc_process.py:135] Launching SC2: /home/oheast/StarCraftII/Versions/Base75689/SC2_x64 -listen 127.0.0.1 -port 20103 -dataDir /home/oheast/StarCraftII/ -tempDir /tmp/sc-arurhi8i/\n",
      "I0922 22:08:28.961961 140235913836352 remote_controller.py:167] Connecting to: ws://127.0.0.1:20103/sc2api, attempt: 0, running: True\n",
      "I0922 22:08:29.965756 140235913836352 remote_controller.py:167] Connecting to: ws://127.0.0.1:20103/sc2api, attempt: 1, running: True\n",
      "I0922 22:08:30.967953 140235913836352 remote_controller.py:167] Connecting to: ws://127.0.0.1:20103/sc2api, attempt: 2, running: True\n",
      "I0922 22:08:34.691793 140235913836352 sc2_env.py:314] Environment is ready\n",
      "I0922 22:08:34.700769 140235913836352 sc2_env.py:507] Starting episode 1: [terran, terran] on Simple64\n",
      "I0922 22:08:51.928241 140235913836352 sc2_env.py:725] Episode 1 finished after 17904 game steps. Outcome: [-1], reward: [-1], score: [7340]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :    1 | Cumulative Reward :   -1 | Epsilon : 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:08:54.362162 140235913836352 sc2_env.py:507] Starting episode 2: [terran, terran] on Simple64\n",
      "I0922 22:09:09.667973 140235913836352 sc2_env.py:725] Episode 2 finished after 17760 game steps. Outcome: [-1], reward: [-1], score: [6950]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :    2 | Cumulative Reward :   -1 | Epsilon : 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:09:12.075554 140235913836352 sc2_env.py:507] Starting episode 3: [terran, terran] on Simple64\n",
      "I0922 22:09:33.794534 140235913836352 sc2_env.py:725] Episode 3 finished after 28368 game steps. Outcome: [-1], reward: [-1], score: [9650]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :    3 | Cumulative Reward :   -1 | Epsilon : 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:09:36.221916 140235913836352 sc2_env.py:507] Starting episode 4: [terran, terran] on Simple64\n",
      "I0922 22:09:57.328722 140235913836352 sc2_env.py:725] Episode 4 finished after 22416 game steps. Outcome: [-1], reward: [-1], score: [8115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :    4 | Cumulative Reward :   -1 | Epsilon : 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:09:59.775833 140235913836352 sc2_env.py:507] Starting episode 5: [terran, terran] on Simple64\n",
      "I0922 22:10:16.816107 140235913836352 sc2_env.py:725] Episode 5 finished after 16272 game steps. Outcome: [-1], reward: [-1], score: [6975]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :    5 | Cumulative Reward :   -1 | Epsilon : 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:10:19.259982 140235913836352 sc2_env.py:507] Starting episode 6: [terran, terran] on Simple64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2181, -0.1539,  0.1357,  0.0586, -0.1497, -0.1870]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:10:42.562938 140235913836352 sc2_env.py:725] Episode 6 finished after 22752 game steps. Outcome: [-1], reward: [-1], score: [8015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :    6 | Cumulative Reward :   -1 | Epsilon : 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:10:45.011965 140235913836352 sc2_env.py:507] Starting episode 7: [terran, terran] on Simple64\n",
      "I0922 22:11:01.991330 140235913836352 sc2_env.py:725] Episode 7 finished after 18768 game steps. Outcome: [-1], reward: [-1], score: [7565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :    7 | Cumulative Reward :   -1 | Epsilon : 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:11:04.427111 140235913836352 sc2_env.py:507] Starting episode 8: [terran, terran] on Simple64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2199, -0.1727,  0.1359,  0.0536, -0.1496, -0.1799]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2280, -0.1936,  0.1545,  0.0429, -0.1691, -0.1732]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2190, -0.2194,  0.1353,  0.0688, -0.2158, -0.1388]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:11:24.469694 140235913836352 sc2_env.py:725] Episode 8 finished after 20592 game steps. Outcome: [-1], reward: [-1], score: [7745]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :    8 | Cumulative Reward :   -1 | Epsilon : 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:11:26.917554 140235913836352 sc2_env.py:507] Starting episode 9: [terran, terran] on Simple64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2191, -0.1959,  0.0899,  0.0509, -0.2213, -0.0672]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:11:49.136475 140235913836352 sc2_env.py:725] Episode 9 finished after 22512 game steps. Outcome: [-1], reward: [-1], score: [8000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :    9 | Cumulative Reward :   -1 | Epsilon : 0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:11:51.584245 140235913836352 sc2_env.py:507] Starting episode 10: [terran, terran] on Simple64\n",
      "I0922 22:12:14.200971 140235913836352 sc2_env.py:725] Episode 10 finished after 22464 game steps. Outcome: [-1], reward: [-1], score: [8170]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.029337894171476364\n",
      "Episode :   10 | Cumulative Reward :   -1 | Epsilon : 0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:12:16.633225 140235913836352 sc2_env.py:507] Starting episode 11: [terran, terran] on Simple64\n",
      "I0922 22:12:36.007200 140235913836352 sc2_env.py:725] Episode 11 finished after 20208 game steps. Outcome: [-1], reward: [-1], score: [8150]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.028260380029678345\n",
      "Episode :   11 | Cumulative Reward :   -1 | Epsilon : 0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:12:38.462901 140235913836352 sc2_env.py:507] Starting episode 12: [terran, terran] on Simple64\n",
      "I0922 22:12:58.233793 140235913836352 sc2_env.py:725] Episode 12 finished after 28800 game steps. Outcome: [0], reward: [0], score: [10850]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.022261464968323708\n",
      "Episode :   12 | Cumulative Reward :    0 | Epsilon : 0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:13:00.671856 140235913836352 sc2_env.py:507] Starting episode 13: [terran, terran] on Simple64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2007, -0.1088,  0.1272,  0.0647, -0.0934, -0.1695]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1804, -0.1281,  0.1563,  0.0555, -0.1284, -0.1882]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:13:17.511600 140235913836352 sc2_env.py:725] Episode 13 finished after 18240 game steps. Outcome: [-1], reward: [-1], score: [6495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.02885523997247219\n",
      "Episode :   13 | Cumulative Reward :   -1 | Epsilon : 0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:13:19.969717 140235913836352 sc2_env.py:507] Starting episode 14: [terran, terran] on Simple64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2020, -0.0787,  0.1340,  0.0572, -0.0914, -0.1658]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2020, -0.0738,  0.1492,  0.0740, -0.0817, -0.1663]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1832, -0.1346,  0.1435,  0.0537, -0.1210, -0.1464]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:13:42.249016 140235913836352 sc2_env.py:725] Episode 14 finished after 22608 game steps. Outcome: [-1], reward: [-1], score: [8515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.027219258248806\n",
      "Episode :   14 | Cumulative Reward :   -1 | Epsilon : 0.997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:13:44.709513 140235913836352 sc2_env.py:507] Starting episode 15: [terran, terran] on Simple64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1642, -0.0822,  0.1658,  0.0569, -0.0684, -0.0898]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:14:08.504465 140235913836352 sc2_env.py:725] Episode 15 finished after 24288 game steps. Outcome: [-1], reward: [-1], score: [8450]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.022505413740873337\n",
      "Episode :   15 | Cumulative Reward :   -1 | Epsilon : 0.997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:14:10.964917 140235913836352 sc2_env.py:507] Starting episode 16: [terran, terran] on Simple64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1720, -0.0865,  0.1525,  0.0333, -0.1020,  0.0022]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:14:26.323168 140235913836352 sc2_env.py:725] Episode 16 finished after 17088 game steps. Outcome: [-1], reward: [-1], score: [6800]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.033908627927303314\n",
      "Episode :   16 | Cumulative Reward :   -1 | Epsilon : 0.997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:14:28.785289 140235913836352 sc2_env.py:507] Starting episode 17: [terran, terran] on Simple64\n",
      "I0922 22:14:45.988636 140235913836352 sc2_env.py:725] Episode 17 finished after 18432 game steps. Outcome: [-1], reward: [-1], score: [7415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.019957367330789566\n",
      "Episode :   17 | Cumulative Reward :   -1 | Epsilon : 0.997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:14:48.434522 140235913836352 sc2_env.py:507] Starting episode 18: [terran, terran] on Simple64\n",
      "I0922 22:15:08.616971 140235913836352 sc2_env.py:725] Episode 18 finished after 20016 game steps. Outcome: [-1], reward: [-1], score: [8000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.02280242182314396\n",
      "Episode :   18 | Cumulative Reward :   -1 | Epsilon : 0.997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:15:11.052618 140235913836352 sc2_env.py:507] Starting episode 19: [terran, terran] on Simple64\n",
      "I0922 22:15:28.689627 140235913836352 sc2_env.py:725] Episode 19 finished after 18624 game steps. Outcome: [-1], reward: [-1], score: [7655]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.014619546942412853\n",
      "Episode :   19 | Cumulative Reward :   -1 | Epsilon : 0.996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:15:31.163301 140235913836352 sc2_env.py:507] Starting episode 20: [terran, terran] on Simple64\n",
      "I0922 22:15:49.765294 140235913836352 sc2_env.py:725] Episode 20 finished after 18816 game steps. Outcome: [-1], reward: [-1], score: [7370]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.021582938730716705\n",
      "Episode :   20 | Cumulative Reward :   -1 | Epsilon : 0.996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:15:52.222511 140235913836352 sc2_env.py:507] Starting episode 21: [terran, terran] on Simple64\n",
      "I0922 22:16:08.192059 140235913836352 sc2_env.py:725] Episode 21 finished after 16944 game steps. Outcome: [-1], reward: [-1], score: [6855]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.024539325386285782\n",
      "Episode :   21 | Cumulative Reward :   -1 | Epsilon : 0.996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:16:10.646609 140235913836352 sc2_env.py:507] Starting episode 22: [terran, terran] on Simple64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1442, -0.0315,  0.1875,  0.0498, -0.0144, -0.1319]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:16:20.249828 140235913836352 sc2_env.py:752] Environment Close\n",
      "I0922 22:16:20.250352 140235913836352 sc2_env.py:752] Environment Close\n",
      "I0922 22:16:20.289821 140235913836352 sc2_env.py:752] Environment Close\n",
      "I0922 22:16:20.290421 140235913836352 sc2_env.py:752] Environment Close\n",
      "I0922 22:16:20.290699 140235913836352 sc2_env.py:752] Environment Close\n",
      "I0922 22:16:30.658726 140235913836352 sc2_env.py:725] Episode 22 finished after 18288 game steps. Outcome: [-1], reward: [-1], score: [7170]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.02715693786740303\n",
      "Episode :   22 | Cumulative Reward :   -1 | Epsilon : 0.996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:16:33.128527 140235913836352 sc2_env.py:507] Starting episode 23: [terran, terran] on Simple64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1307, -0.0217,  0.1951,  0.0311, -0.0102, -0.1157]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1188,  0.0008,  0.1981,  0.0290, -0.0064, -0.0841]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1367,  0.0268,  0.2016,  0.0319,  0.0113, -0.0837]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:16:51.565691 140235913836352 sc2_env.py:725] Episode 23 finished after 18864 game steps. Outcome: [-1], reward: [-1], score: [7540]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.025950733572244644\n",
      "Episode :   23 | Cumulative Reward :   -1 | Epsilon : 0.996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:16:54.022082 140235913836352 sc2_env.py:507] Starting episode 24: [terran, terran] on Simple64\n",
      "I0922 22:17:03.444390 140235913836352 sc2_env.py:725] Episode 24 finished after 12240 game steps. Outcome: [1], reward: [1], score: [6705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.024903349578380585\n",
      "Episode :   24 | Cumulative Reward :    1 | Epsilon : 0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:17:05.911940 140235913836352 sc2_env.py:507] Starting episode 25: [terran, terran] on Simple64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1309, -0.0034,  0.1979,  0.0218, -0.0222, -0.0160]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:17:24.015350 140235913836352 sc2_env.py:725] Episode 25 finished after 18384 game steps. Outcome: [-1], reward: [-1], score: [7185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.02123051881790161\n",
      "Episode :   25 | Cumulative Reward :   -1 | Epsilon : 0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:17:26.471603 140235913836352 sc2_env.py:507] Starting episode 26: [terran, terran] on Simple64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1284,  0.0189,  0.1998,  0.0097, -0.0245, -0.0777]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:17:43.552594 140235913836352 sc2_env.py:725] Episode 26 finished after 16320 game steps. Outcome: [-1], reward: [-1], score: [6900]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.016040977090597153\n",
      "Episode :   26 | Cumulative Reward :   -1 | Epsilon : 0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:17:46.006954 140235913836352 sc2_env.py:507] Starting episode 27: [terran, terran] on Simple64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1223,  0.0059,  0.2062,  0.0236,  0.0053, -0.0933]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:18:07.904067 140235913836352 sc2_env.py:725] Episode 27 finished after 22032 game steps. Outcome: [-1], reward: [-1], score: [7560]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.017825432121753693\n",
      "Episode :   27 | Cumulative Reward :   -1 | Epsilon : 0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:18:10.365412 140235913836352 sc2_env.py:507] Starting episode 28: [terran, terran] on Simple64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1227, -0.0127,  0.1921,  0.0521, -0.0143, -0.0992]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1195,  0.0104,  0.2046,  0.0243,  0.0052, -0.0820]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:18:31.283363 140235913836352 sc2_env.py:725] Episode 28 finished after 22848 game steps. Outcome: [-1], reward: [-1], score: [8490]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.01611233502626419\n",
      "Episode :   28 | Cumulative Reward :   -1 | Epsilon : 0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:18:33.751149 140235913836352 sc2_env.py:507] Starting episode 29: [terran, terran] on Simple64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1112,  0.0145,  0.2061,  0.0211,  0.0091, -0.0768]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1221,  0.0244,  0.2301,  0.0184,  0.0135, -0.0538]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:18:50.094068 140235913836352 sc2_env.py:725] Episode 29 finished after 17808 game steps. Outcome: [-1], reward: [-1], score: [7600]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.018153680488467216\n",
      "Episode :   29 | Cumulative Reward :   -1 | Epsilon : 0.994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 22:18:52.566709 140235913836352 sc2_env.py:507] Starting episode 30: [terran, terran] on Simple64\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Winning rate graph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "SCORE_FILE = 'rlagent_with_vanilla_dqn_score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SCORE_FILE + '.txt', \"rb\") as fp:\n",
    "    scores = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0., 100.],\n",
       "       [  0.,   0., 100.],\n",
       "       [  0.,   0., 100.],\n",
       "       ...,\n",
       "       [  9.,  12.,  79.],\n",
       "       [  9.,  12.,  79.],\n",
       "       [  9.,  12.,  79.]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_scores = np.array(scores)\n",
    "np_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(np_scores)), np_scores.T[0], color='r', label='win rate')\n",
    "plt.plot(np.arange(len(np_scores)), np_scores.T[1], color='g', label='tie rate')\n",
    "plt.plot(np.arange(len(np_scores)), np_scores.T[2], color='b', label='lose rate')\n",
    "plt.ylabel('Score %')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "starcraft2",
   "language": "python",
   "name": "starcraft2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
